import torch
from unsloth import FastLanguageModel
from transformers import pipeline

# è·¯å¾‘ç‚ºä½ å‰›å‰›è¨“ç·´å¾Œå„²å­˜çš„æ¨¡å‹è³‡æ–™å¤¾
model_path = "./llama3-finetuned"
max_seq_length = 2048
dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16

# 1. è¼‰å…¥æ¨¡å‹èˆ‡ tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_path,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=True,  # è‹¥ä½¿ç”¨äº† 4-bit å¾®èª¿
)

# 2. å•Ÿç”¨æ¨è«–æ¨¡å¼ï¼ˆæœƒåŠ é€Ÿé‹ç®—ï¼‰
FastLanguageModel.for_inference(model)

# 3. ä½¿ç”¨ HuggingFace pipeline ç”¢ç”Ÿæ–‡å­—
pipe = pipeline(
    task="text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1,
)

# 4. æ¸¬è©¦ç”Ÿæˆç¯„ä¾‹
prompt = "Once upon a time, in a world of dragons and wizards,"
output = pipe(
    prompt,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7,
    top_p=0.95,
    repetition_penalty=1.2,
    eos_token_id=tokenizer.eos_token_id,
)

# 5. è¼¸å‡ºçµæœ
print("\nğŸ“œ Generated Text:")
print(output[0]["generated_text"])
